%Made By Thomas Debelle fuck Thomas
\documentclass{report}
\usepackage[a4paper, total={6in, 9in}]{geometry}
\usepackage[utf8]{inputenc}
\usepackage[french]{babel}
\usepackage{graphicx}
\usepackage{graphics}
\usepackage{colortbl}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{amssymb}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{array}
\usepackage{float}
\usepackage{amsfonts}
\usepackage{tikz}
\usepackage{fancyhdr}
\usepackage{titlesec}
\usepackage{xparse}
\usepackage{wrapfig}

\hypersetup{
    colorlinks=true,
    linkcolor=black,
    filecolor=magenta,
    urlcolor=cyan,
    pdftitle={Overleaf Example},
    pdfpagemode=FullScreen,
    }
\begin{document}


\begin{titlepage}
    \begin{figure}
        \includegraphics[height = 2cm]{UCL_Logo.png}
        \label{fig:my_label}
    \end{figure}

    \hspace*{100cm}
    \centering
    \vspace*{7cm}

    {\Huge \textbf{Résumé de LEPL1106}}\\
    \vspace*{0.25cm}
    compilation du \today\\
    \vspace*{0.25cm}
    \Large{Thomas Debelle}\\

    \vspace*{9.5cm}
    {\Large Juin 2023}
\end{titlepage}


\tableofcontents
\newpage

\section*{Préface}

Bonjour à toi !\\

Cette synthèse recueille toutes les informations importantes données au cours, pendant les séances de tp et est améliorée grâce au note du Syllabus. Elle ne remplace pas le cours donc écoutez bien les conseils et potentielles astuces que les professeurs peuvent vous donner. Notre synthèse est plus une aide qui, on l'espère, vous sera à toutes et tous utile.\\

Elle a été réalisée par toutes les personnes que tu vois mentionnées. Si jamais cette synthèse a une faute, manque de précision, typo ou n'est pas à jour par rapport à la matière actuelle ou bien que tu veux simplement contribuer en y apportant tes connaissances ? Rien de plus simple ! Améliore la en te rendant \href{http://www.github.com/Tfloow/Q4_EPL}{ici} où tu trouveras toutes les infos pour mettre ce document à jour. (\textit{en plus tu auras ton nom en gros ici et sur la page du github})\\

Nous espérons que cette synthèse te sera utile d'une quelconque manière ! Bonne lecture et bonne étude.


\part{Signaux}

\chapter{Les signaux}
\section{Définition}
\subsubsection{Définition}
Un signal est une fonction de une ou plusieurs variables (continues ou discrètes) qui correspondent à de l'information ou a un phénomène physique.

\subsubsection{Continues ou discrets ?}
Un signal est dit continu si il est définit sur un espace de temps continu. On note ce signal $x(t)$. Et il est dit discret si il est définit sur un espace discret de temps. On note ce signal $x[t]$.

\subsubsection{Manipulation des signaux}
Pour le cas discrets ou continu, nous pouvons réaliser les opérations suivantes.
\begin{itemize}
\item Combinaison linéaire $\rightarrow \alpha_{1}x_{1}(t) + \alpha_{2}x_{2}(t)$
\item Multiplication $ \rightarrow x_{1}[t]x_{2}[t]$
\item Dilatation $ \rightarrow x[n/a], a > \mathbb{R}$
\item Translation $ \rightarrow x(t - t_0), t_0 \in \mathbb{R}$
\item Renversement $ \rightarrow x(-t)$
\item Différentiation (que pour le cas continu) $ \rightarrow \frac{d^n x(t)}{dt^n}$
\item Intégration (que pour le cas continu) $\rightarrow \int x(t) dt$
\end{itemize}

\section{Signaux élémentaires}
\subsection{Signaux exponentiels}
Pour les signaux continus nous avons:
\begin{equation}
x(t) = B e^{at}
\end{equation}
Et pour les signaux discrets nous avons:
\begin{equation}
x[n] = Br^n \rightarrow 0 < r < 1
\end{equation}

\subsection{Signaux sinusoïdales}
Pour les signaux continus nous avons:
\begin{equation}
x(t) = A cos(\omega t + \phi)
\end{equation}
Et pour les signaux discrets nous avons:
\begin{equation}
x[n] = A cos(\Omega n + \Phi)
\end{equation}
Il a une période de $\Omega N = 2 \pi m$

\subsection{Signaux amortis}
Pour les signaux continus et avec $\alpha > 0$:
\begin{equation}
x(t) = A e^{-\alpha t}cos(\omega t + \phi)
\end{equation}
Et pour les signaux discrets:
\begin{equation}
x[n] = Br^ncos(\Omega n + \Phi)
\end{equation}

\subsection{L'impulsion (temps discret)}
Comme son nom l'indique, ce signal se représente sous la forme d'une impulsion. Par sa définition, cela nous force a avoir un signal discret !
\begin{equation}
\begin{cases}
\delta [n] = 1 \rightarrow n = 0 \\
\delta [n] = 0 \rightarrow \forall n \notin 0
\end{cases}
\end{equation}
On peut réaliser des impulsions décaler en écrivant $\delta [n-x]$ avec $x$ représentant la valeur du décalage.

\subsection{L'échelon (temps discret)}
Ce type de signal élémentaire est encore plus trivial puisqu'il se résume à:
\begin{equation}\label{eq:1}
\begin{cases}
1 \rightarrow n \geq 0 \\
0 \rightarrow n < 0
\end{cases}
\end{equation}
On peut aussi voir l'échelon comme une somme infinie d'impulsion comme $\sum_{k \geq 0}^{\infty} \delta[n-k]$.

\subsection{L'impulsion (temps continu)}
\begin{equation}
\begin{cases}
\delta (t) = 0 \text{ si } t \neq 0\\
\delta (0) = (+\infty)\\
\int_{-a}^a \delta (s) ds = 1 \rightarrow \forall a > 0
\end{cases}
\end{equation}
A noter que la dernière ligne nous crée une propriété bien spécifique. En effet, la valeur de l'impulsion est limité par les bornes $a$ puisqu'on impose une intégrale égale à 1.

\subsubsection{Lien entre impulsion et échelon}
$\delta (t) = u'(t)$ donc l'impulsion est une sorte de dérivé de l'échelon. ceci nous permet également d'obtenir cette formule:
\begin{equation}
\int_{-\infty}^{\infty} x(s) \delta(s)ds = x(0)
\end{equation}
On prouve cela de manière \textit{peu rigoureuse} en remarquant que: pour $s \neq 0$, on a $\delta(s) = 0$ donc $x(s)\delta(s) = 0 = x(0)\delta(s)$ finalement on a:
\begin{equation}
\int_{-\infty}^{\infty} x(s) \delta(s)ds = \int_{-\infty}^{\infty} x(0) \delta(s)ds = x(0) \int_{-\infty}^{\infty} \delta(s)ds = x(0)
\end{equation} 

\subsubsection{Décomposition en impulsions}
On peut décomposer tout signal en impulsion comme ci-dessous:
\begin{equation}
\begin{cases}
\text{temps discret } \Rightarrow x[n] = \sum_{k=-\infty}^{\infty}x[k]\delta[n-k] \\
\text{temps continu } \Rightarrow x(t) = \int_{-\infty}^{\infty} x(s) \delta(t-s)ds
\end{cases}
\end{equation}



\chapter{Fourier}
\section{La représentation de Fourier}

\subsection{Signaux continus}
\subsubsection{Stocker un signal efficacement}
Un signal est composé de sinus et de cosinus à des amplitudes, phases et fréquences différentes.\\
La manière la plus efficace pour stocker un signal est d'avoir pour chaque fréquence \textit{multiple} de la fréquence de base $\omega_0$ son amplitude. (on verra que en effet, on peut faire cette supposition que chaque fréquence sont des multiples de fréquences) Cela ressemble donc à ça:
\begin{align}
x(t) &= \textcolor{red}{1}cos(\textcolor{red}{1}\omega_0t) + \textcolor{blue}{2}sin(\textcolor{blue}{2}\omega_0t) + \textcolor{red}{\frac{1}{2}}cos(\textcolor{red}{2}\omega_0t) + \textcolor{blue}{\frac{1}{2}}sin(\textcolor{blue}{3}\omega_0t) +  \textcolor{red}{\frac{4}{5}}cos(\textcolor{red}{4}\omega_0t) \label{eq:signal}\\
cos &= [\textcolor{red}{1}, \textcolor{red}{\frac{1}{2}}, \textcolor{red}{0}, \textcolor{red}{\frac{4}{5}}]\\
sin &= [\textcolor{blue}{0}, \textcolor{blue}{2}, \textcolor{blue}{\frac{1}{2}}]
\end{align}
\subsubsection{Périodicité}
On sait que $x(t)$ est \textcolor{blue}{périodique} et d'\textit{énergie finie}.(Périodicité de $\omega_0 = \frac{1}{T_0}$) On peut décomposer notre signal en \textbf{Série de Fourier \textit{trigonométrique}}:
\begin{align}
x(t) &= a_0 + \sum_{k=1}^{+\infty} \Bigl(2 a_k cos(k\omega_0 t) + 2b_k sin(k \omega_0 t) \Bigl)\\
a_k &= \frac{1}{T_0} \int_0^{T_0} x(t) cos(k\omega_0t)dt\\
b_k &= \frac{1}{T_0} \int_0^{T_0} x(t) sin(k\omega_0t)dt
\end{align}
De plus, on peut toujours décomposer un signal \textit{périodique} en une partie \textbf{impaire} et \textbf{paire}. (en Fourier, il suffit de prendre la partie $cos$ donc paire et $sin$ donc impaire)\\

Chose importante à remarquer, on va souvent tracer des graphes avec un axe y de type $2a[n]$. Ce 2 est un des prémices de Fourier complexe.

\subsubsection{Signaux carrés}
Un signal carré est exprimé comme ci-dessous avec une période de $2\pi$:
\begin{align}
x(t) = \begin{cases}
1 \qquad &\text{si } 0 \leq t < \pi\\
-1 \qquad &\text{sinon}
\end{cases}
\end{align}
Si on fait la série de Fourier, on peut se rendre compte que ce signal est composé d'une \textit{infinité} de sinus de tel sorte que:
\begin{equation}
x(t) = \sum_0^{+\infty} \frac{1}{2n+1} sin((2n+1)t)
\end{equation}

\subsubsection{Fourier Complexes}
On voit que cette série est bien plus simple et est une révolution pour calculer \textit{Fourier}:
\begin{align}
x(t) &= \sum_{k = - \infty}^{+ \infty} X_k e^{jk\omega_0 t}\\
X_k &= \frac{1}{T_0} \int_0^{T_0} x(t) e^{-jk\omega_0t}dt \label{eq:coefComplexe}
\end{align}
Donc on prend le conjugué pour calculer $X_k$. C'est ici que le coefficient $2$ fait du sens, on se rappelle les formules d'Euler.
\begin{equation}
cos(t) = \frac{e^{jt} + e^{-jt}}{2} \qquad \qquad sin(t) = \frac{e^{jt} - e^{-jt}}{2j}
\end{equation}
Donc le lien avec la série en Réelle est: $X_0 = a_0$, $X_k = X_{-k}^{*} = a_k -jb_k$. On trouve cela en injectant dans l'équation \ref{eq:coefComplexe} la formule d'Euler.

\subsubsection{Représentation de Fourier} %télécharger les images des slides à jour
Pour faire la représentation de Fourier, on va commencer par prendre l'équation \ref{eq:signal} et la transformer:
\begin{align}
x(t) &= 2(\textcolor{red}{\frac{1}{2}}cos(\textcolor{red}{1}\omega_0t) + \textcolor{red}{\frac{1}{4}}cos(\textcolor{red}{2}\omega_0t) + \textcolor{red}{\frac{4}{10}}cos(\textcolor{red}{4}\omega_0t)) + 2(\textcolor{blue}{1}sin(\textcolor{blue}{2}\omega_0t)  + \textcolor{blue}{\frac{1}{4}}sin(\textcolor{blue}{3}\omega_0t)) \label{eq:signalT}
\end{align}
Ensuite on injecte la formule d'Euler dans l'équation \ref{eq:signalT} ce qui donne:
\begin{figure}[H]
\centering
\includegraphics[width=8cm]{img/signalTE.png} %oui c'est une photo donc si quelqu'un est chaud ré-écrire proprement l'équation
\includegraphics[width=6cm]{img/grapheSignalTE.png}
\end{figure}
Notre signal $x(t)$ est bien défini de manière unique via ses coefficients !


\subsubsection{Base orthogonale}
Les exponentielles complexes de Fourier sont orthogonales dans l'espace de Hilbert $\mathcal{H}_T$ équipé avec le produit scalaire:
\begin{align}
\langle x, y\rangle &= \frac{1}{T_0}\int_0^{T_0}x(t)y^{*}(t)dt\\
\{e^{jk\omega_0t}\}&, k \in \mathbb{Z} \rightarrow \textbf{ est }\perp
\end{align}

\subsection{Signaux discrets}
La périodicité d'un signal change puisqu'on ne peut avoir que des valeurs $\mathbb{N}$. En effet, on dit qu'un signal est de période N si $x[n + N] = x[n]$. De plus, N est en $[rad]$ et pas $[\frac{rad}{s}]$ car la \textcolor{blue}{pulsation fondamentale} est $\Omega_0 = \frac{2\pi}{N}$

\subsubsection{Calcul de la série de Fourier}
\begin{align}
x[n] &= \sum_{k=0}^{N-1} X[k] e^{jk\Omega_0n}\\
X[k] &= \frac{1}{N} \sum_{m=0}^{N-1} x[m] e^{-jk\Omega_0m}
\end{align}
On a seulement besoin de $N$ coefficient grâce à la \textbf{périodicité}. En effet, $e^{j(N+k)\Omega_0n} = e^{jk\Omega_0n}$



\section{La transformée de Fourier}
Cela s'applique sur les signaux \textit{non-périodiques} et \textit{continus}.\\
Il faut donc voir le signal comme ayant un $\omega = 0 = \frac{1}{T_0} = \frac{1}{\infty}$ et y appliquer la série de Fourier!

\subsection{Calcul de la transformée}
On dit que le \textcolor{blue}{spectre} $X(j\omega)$ du signal $x(t)$ est sa \textbf{transformée de Fourier}.
\begin{align}
X(j\omega) &= \int_{-\infty}^{\infty} x(t) e^{-j \omega t}dt\\
x(t) &= \frac{1}{2\pi} \int_{-\infty}^{\infty} X(j\omega) e^{j\omega t} d\omega \label{eq:invF}
\end{align}
On appelle l'équation \ref{eq:invF}, la transformée de Fourier \textbf{inverse}.\\

Pour arriver à ce résultat, nous avons réalisés quelques modifications à la série de Fourier:
\begin{align}
\omega_k &= k \omega_0 = \frac{2 \pi k}{T_0} \Rightarrow \omega_{k+1} - \omega{k} = \Delta \omega = \frac{2\pi}{T_0}\\
\int_0^{T} & \approx \int_{\frac{-T}{2}}^{\frac{T}{2}} \qquad \text{ Pour "couvrir" tout le domaine}\\
x(t) &= \sum_{-\infty}^{\infty} X_k e^{j \omega_0 k t}\\
	 &= \sum_{-\infty}^{\infty} \Bigl \{ \frac{1}{T_0} \int_{\frac{-T_0}{2}}^{\frac{T_0}{2}} x(t) e^{-j k \omega_0 t} dt \Bigl \} e^{j k \omega_0 t}\\
	 &= \sum_{-\infty}^{\infty} \frac{1}{2 \pi} \int_{\frac{-T_0}{2}}^{\frac{T_0}{2}} x(t) e^{-j k \omega_0 t} dt\quad e^{j k \omega_0 t} \Delta \omega \label{eq:finalF}
\end{align} 
On remarque qu'on a 2 parties intéressantes dans l'équation \ref{eq:finalF}. On a une partie qui évolue dans le temps, c'est notre \textit{signal continu}, c'est notre \textcolor{blue}{spectre}. L'autre partie dépend de la fréquence et c'est la \textit{reconstruction du signal}. 
\begin{figure}[H]
\centering
\includegraphics[width=7cm]{img/reconstructionF.png}
\caption{à gauche, un signal pair \qquad à droite, le spectre}
\end{figure}

\subsection{Calcul discret}
On a un signal $x[n]$ qui s'exprime de manière unique selon ses fréquences:
\begin{align}
x[n] &= \frac{1}{2\pi}\int_{-\pi}^{\pi} X \bigl(e^{j\Omega} \bigl) e^{j\Omega n} d\omega\\
X\bigl(e^{j\Omega} \bigl) &= \sum_{m=-\infty}^{\infty} x[m]e^{-j\Omega m} \label{eq:transfoDF}
\end{align}
L'équation \label{eq:transfoDF} donne la \textcolor{blue}{transformée} de Fourier.


\section{Propriétés de Fourier} \label{proprF}

\subsection{Dualité} \label{Dua}
\begin{align}
x(t) & \longleftrightarrow X(j\omega)\\
X(jt) & \longleftrightarrow 2\pi x(-\omega)
\end{align}
Donc on échange $t \rightarrow -\omega$ et $\omega \rightarrow t$ et on échange les fonctions.
\subsubsection{Démonstration}
\begin{align*}
X(j \omega) &= \int_{-\infty}^{\infty} x(t) e^{-j\omega t} dt\\
x(t) &= \frac{1}{2\pi} \int_{\infty}^{\infty} X(j\omega) e^{j\omega t} d\omega\\
x(-t) &= \frac{1}{2\pi} \int_{\infty}^{\infty} X(j\omega) e^{-j\omega t} d\omega\\
2 \pi x(-t) &= \int_{\infty}^{\infty} X(j\omega) e^{-j\omega t} d\omega\\
2 \pi x(-\omega) &= \int_{\infty}^{\infty} X(jt) e^{-j\omega t} dt
\end{align*}

\subsection{Linéarité}
En effet,
\begin{align*}
x_k(t) & \longleftrightarrow X_k(j\omega)\\
\sum_k x_k(t) & \longleftrightarrow \sum_k X_k(j\omega)
\end{align*}

\subsection{Translation}
On a:
\begin{align*}
x(t) & \longleftrightarrow X(j\omega)\\
x(t-t_0) & \longleftrightarrow X(j\omega)e^{-j \omega t_0}
\end{align*}
Il faut bien retenir qu'une translation en temporelle amène à une exponentielle en phasorielle. Combiner à la propriété de dualité venant de \ref{Dua} est très puissant et utile.\\
On doit aussi noter que cela ne change \textbf{pas} l'amplitude du spectre mais change sa \textbf{phase}.

\subsection{Modulation}
C'est quand on combine la translation et la dualité:
\begin{align*}
x(t) & \longleftrightarrow X(j\omega)\\
e^{j\omega_0 t}x(t) & \longleftrightarrow X(j(\omega - \omega_0))
\end{align*}
Cette modulation va elle modifier l'amplitude du signal car déplace son "espace" phasoriel revient à déplacer ses fréquences et en ajoute.
\begin{align*}
x(t) & \longleftrightarrow X(j\omega)\\
cos(\omega_0 t)x(t) \longleftrightarrow \frac{1}{2}(X(j(\omega - \omega_0)) + X(j(\omega + \omega_0))) 
\end{align*}
On reconnait clairement les formules d'Euleur.

\subsection{Différentiation}
Dériver un signal à \textbf{énergie finie} revient à \textcolor{blue}{amplifier} ses \textcolor{blue}{hautes} fréquences.
\begin{align*}
x(t) \longleftrightarrow X(j\omega)\\
\frac{d^kx}{dt^k}(t) \longleftrightarrow (j\omega)^kX(j\omega)
\end{align*}
\begin{figure}[H]
\centering
\includegraphics[width=10cm]{img/diff.png}
\caption{Exemple de transformation de Fourier}
\end{figure}

\subsection{Multiplication par un monôme}
Par dualité, faire ceci:
\begin{align*}
x(t) \longleftrightarrow X(j\omega)\\
t^nx(t) \longleftrightarrow j^n \frac{d^nX}{d\omega^n} (j\omega)
\end{align*}
\subsection{Intégration}
Donc intégrer, augmente les \textcolor{blue}{basses} fréquences:
\begin{align*}
x(t) \longleftrightarrow X(j\omega)\\
\sum_{-\infty}^t	x(t)dt \longleftrightarrow \frac{X(j\omega}{j\omega} + \textcolor{red}{\pi X(0) \delta(\omega)}
\end{align*}
La partie en rouge est dû au fait qu'un résultat d'une intégration à toujours une \textcolor{red}{constante d'intégration} (le fameux $+C$)

\subsection{Dilatation}
Quand on \textit{dilate} un signal, on \textit{compresse} son espace spectre.
\begin{align*}
x(t) \longleftrightarrow X(j\omega)\\
x(t/a) \longleftrightarrow |a|X(aj\omega)
\end{align*}
Un truc utile est de penser au vidéo au ralenti où on \textit{dilate} le temps et on \textit{compresse} son spectre donc on entend plus les \textit{basses fréquences}.

\subsubsection{Parseval}
On \textbf{conserve} l'énergie du signal grâce à son spectre. donc:
\begin{align*}
x(t) \longleftrightarrow X(j\omega)\\
\int_{-\infty}^{\infty} |x(t)|^2 dt \longleftrightarrow \frac{1}{2\pi} \int_{-\infty}^{\infty} |X(j\omega)|^2 d\omega
\end{align*}

\subsection{Renversement}
Renverser un signal revient à renverser son spectre.
\begin{align}
x(t) \longleftrightarrow X(j\omega)\\
x(-t) \longleftrightarrow X(-j\omega)
\end{align}
C'est une dilatation par $a = -1$.
\subsubsection{Complexe conjugué}
Le spectre d'un \textcolor{blue}{signal conjugué} est le \textcolor{blue}{conjugué renversé} du spectre du signal.
\begin{align*}
x(t) \longleftrightarrow X(j\omega)\\
x^*(t) \longleftrightarrow X^*(-j\omega)
\end{align*}


\section{Transformée usuelle}
On utilisera des propriétés vu à la section \ref{proprF}.

\subsubsection{Delta de Dirac} \label{Dirac}
C'est plutôt simple, il faut appliquer simplement la formule:
\begin{equation}
\begin{cases}
\delta(t) = 0 \text{ si } t \neq 0\\
\int_a^{-a} \delta(t) dt = 1 \forall a > 0\\
\int_{-\infty}^{\infty} x(t) \delta(t) dt = x(0)
\end{cases}
\end{equation}
Via toutes ces propriétés on voit facilement que:
\begin{equation}
X(j\omega) = \int_{-\infty}^{\infty} \delta(t) e^{-j \omega t} dt = 1(e^0) = 1
\end{equation}

\subsubsection{Delta de Kronecker}
Même idée que à \ref{Dirac} si ce n'est qu'on est en discret. Le résultat reste le même.

\subsection{Train d'impulsions (peigne) de Dirac}
On a donc une fonction qui ressemble à cela: (d'où le nom)
\begin{figure}[H]
\centering
\includegraphics[width=7cm]{img/peigne.png}
\caption{peigne de Dirac (avec un petit spoil)}
\end{figure}
formellement un peigne de Dirac est:
\begin{equation}
x(t) = \sum_{k \in \mathbb{Z}} \delta(t-kT_0)
\end{equation}
et son $X(j\omega)$ est:
\begin{equation}
x(t) \longleftrightarrow X(j\omega) = \omega_0 x_{\omega_0} (\omega) \text{ avec } \omega_0 = \frac{2\pi}{T_0}
\end{equation}

\subsubsection{Preuve}
\begin{align*}
X(j\omega) &= \int_{-\infty}^{\infty} x(t) e^{-j \omega t} dt\\
&= \sum_{k = -\infty}^{\infty} \int_{-\infty}^{\infty} \delta(t- kT_0) e^{-j \omega t} dt\\
&= \sum_{k = -\infty}^{\infty} e^{-j \omega k T_0} \text{ car on a une énergie finie}\\
\mathcal{F}(\delta(t-kT_0)) &= e^{-j\omega k T_0}
\end{align*}
Ensuite, on va refaire une série de Fourier:
\begin{align*}
x(t) &= \sum_{l = -\infty}^{\infty} X_l e^{j\omega_0 t} & X_l &=  \frac{1}{T_0} \int_{-\frac{T_0}{2}}^{\frac{T_0}{2}} x(t) e^{-j \omega_0 lt} dt\\
&= \frac{1}{T_0} \sum_{l = -\infty}^{\infty} e^{j\omega_0 lt} & &= \frac{1}{T_0} \int \delta(t) e^{-j \omega_0 lt} dt\\
& & &= \frac{1}{T_0}
\end{align*} %pas sûr sûr pour cette démo je doisd recheck
Ensuite, on utiliser la dualité:
\begin{align*}
x(t) &\longleftrightarrow X(j\omega)\\
X(jt) &\longleftrightarrow 2\pi x(-\omega)\\
\mathcal{F}(e^{j\omega_0 lt}) &= 2 \pi \delta(\omega - \omega_0 l)\\
x(t) = \frac{1}{T_0} \sum_{l = - \infty}^{\infty} 2 \pi \delta(\omega - l \omega_0) &= \omega_0 \sum (\delta(\omega -l \omega_0) = \omega_0 x(\omega)
\end{align*}

\subsection{Fonction échelon}
\begin{figure}[H]
\centering
\includegraphics[width=10cm]{img/echeF.png}
\caption{Transformé de la fonction échelon}
\end{figure}
Sa transformée de Fourier est:
\begin{equation}
x(t) = u(t) \longleftrightarrow X(j\omega) = \pi \delta(t) + \frac{1}{j \omega}
\end{equation}

\subsubsection{Preuve}
On utilise l'intégration car on sait que:
\begin{align*}
u(t) &=  \int_0^{\infty} \delta(t) dt\\
\mathcal{F}(\delta(t)) &= 1\\
u(j\omega) &= \frac{1}{j \omega} + \pi \delta(\omega)
\end{align*}

\subsection{Fonction fenêtre} \label{window}
Cette fonction, correspond à 2 impulsion. tel que:
\begin{equation}
imp = u(t+1) - u(t-1)
\end{equation}
Ces fonctions sont des filtres \textit{passes-bandes} et à pour transformé de Fourier:
\begin{align*}
x(t) = \Pi (t) \longleftrightarrow X(j\omega) = 2 sinc(t) = 2\frac{sin(\omega)}{\omega}
\end{align*}

\subsubsection{Preuve}
On utilise l'\textit{addition} de 2 échelons \textit{translatés}.
%Refaire toute la preuve

\subsection{Fonction signe}
La fonction signe est:
\begin{equation}
sign(t) = \begin{cases}
-1 \quad t < 0\\
0 \quad t = 0\\
1 \quad t > 0
\end{cases}
\end{equation}
et à pour transformée de Fourier:
\begin{align*}
x(t) = sign(t) = \longleftrightarrow X(j\omega) = \frac{2}{j\omega}
\end{align*}
Cela se prouve avec la dilatation et la translation de la fonction échelon.

\section{Tableau de Check}
%insérer tableau slide 22 cours 4

\chapter{Filtrage et Bode}

\section{Filtre}
On se souvient, que convoluer 2 signaux revient à multiplier leur spectre. De plus, dans un système \textbf{LIT} le résultat d'un signal x est: $y = h\ast x$. Donc, la plupart du temps quand un programme informatique fait un système LIT, il va faire la \textit{transformée de Fourier} pour simplement multiplier et pas avoir des sommes de multiplications.\\
Donc en Résumé:
\begin{align*}
Y(t) = H(t) \ast X(t) &\longleftrightarrow Y(j\omega) = H(j \omega) X(j \omega)\\
=|H(j \omega)||X(j \omega)|&e^{j(arg\{H(j\omega )\}+ arg\{X(j \omega) \}}
\end{align*}
Donc, il parait clair qu'un système modifie le contenu fréquentiel de notre signal $X$.
\begin{figure}[H]
\centering
\includegraphics[width=8cm]{img/signalFreq.png}
\caption{Exemple de système LIT}
\end{figure}
Le fait de modifier le \textit{contenu fréquentiel} est appelé le \textbf{filtrage}.
\subsection{Type de filtre}
Il existe 3 grands types de filtres:
\begin{enumerate}
\item \textcolor{red}{Les filtres passe-bas}: ils vont atténuer et annuler les \textit{hautes-fréquences} pour ne laisser passer que les \textit{basses}.
\item \textcolor{red}{Les filtres passe-haut}: ils vont atténuer et annuler les \textit{basses fréquences} et ne laisser passer que les \textit{hautes}.
\item \textcolor{red}{Les filtres passe-bande}: ils ne vont laisser passer qu'une partie précise de fréquence. On peut voir cela comme la différence entre un filtre passe-bas d'envergure $A$ et un filtre passe-base d'envergure $B$ avec $A>B$. La différence $A-B$ est appelé la \textbf{largeur de bande}. 
\end{enumerate}
C'est grâce à cela que la radio fonctionne ou que les filtres photo fonctionnent.\\

\newpage % à enlever si écart 

\subsubsection{Les circuits RC}
Parmi les exemples typiques, on peut s'intéresser au circuit RC (cf: \href{https://github.com/Tfloow/Q4_EPL/blob/main/SynthèseCompilée/LELEC1370.pdf}{LELEC1370}). 
\begin{wrapfigure}{r}{.5\textwidth}
\centering
\includegraphics[width=5cm]{img/RC.png}
\caption{RC classique}
\end{wrapfigure}
En effet, on peut modéliser les équations d'un tel circuit comme suit:
\begin{align*}
x(t) &= R C \frac{du}{dt} + u(t) \longleftrightarrow\\
X(j \omega) = RC j \omega Y(j \omega) + Y(j \omega) &= Y(j \omega) (j\omega RC+1)\\
\frac{Y(j \omega)}{X(j \omega)} = H(j \omega) &= \frac{1}{1+ j \omega RC}\\
\text{Si } \omega_c = \frac{1}{RC}; |H(j\omega)| &= \frac{1}{\sqrt{1 + j(\omega / \omega_c)^2}}
\end{align*}
Ici, notre $\omega_c$ représente la \textcolor{blue}{fréquence de coupure}. C'est donc la valeur à partir de laquelle, on aura un changement notable de résultat.\\
Une chose à remarquer est que le déphasage lié à la fréquence angulaire augmente jusqu'à $\frac{\pi}{2}$. Peu perceptible au début, cela ne change rien fondamentalement.

\subsection{Filtres non-idéaux}
Un filtre parfait serait un filtre qui soit: nulle partout où on ne veut pas la fréquence et unitaire partout où on la veut.\\
\begin{wrapfigure}{r}{.5\textwidth}
\centering
\includegraphics[width=6cm]{img/sinc.png}
\caption{Sinus cardinal}
\end{wrapfigure}
Mais comme vu au point \ref{window}, un tel signal en phasoriel correspond à:
\begin{align*}
H(j\omega) \longleftrightarrow h(t) = \frac{sin(\omega_c t)}{\pi t}
\end{align*}
Le gros soucis avec cela, c'est qu'on a une réponse actuelle du système qui dépend de quelque chose se passant dans le \textit{futur} donc pas vraiment physique. Le système n'est pas \textit{causal}.\\
On peut aussi noter qu'on aura un déphasage nul. En réalité, on cherche à avoir une déphasage linéaire car c'est simplement une modulation et est plus facilement corrigeable.

\subsubsection{Passe-bande non-idéal}
\begin{wrapfigure}{r}{.5\textwidth}
\centering
\includegraphics[width=6cm]{img/pbNonIde.png}
\end{wrapfigure}
Il en va de même pour le filtre passe-bande qui est composé de deux fonctions fenêtres comme suit:
\begin{align*}
|H(j\omega)| &= \sqcap \bigr(\frac{\omega + \omega_0}{B/2}\bigr) + \sqcap \bigr(\frac{\omega - \omega_0}{B/2}\bigr)\\
h(t) &= \frac{sin(Bt/2}{\pi t}2 cos(\omega_0 t)
\end{align*}
Comme au point précédent, c'est la non causalité et donc le manque de sens physique qui rend ce type de filtres idéaux impossibles.

\subsection{Filtre analogique}
Ce sont des filtres composés d'éléments \textcolor{blue}{passifs} et/ou \textcolor{blue}{actifs}. 
\begin{figure}[H]
\centering
\includegraphics[width=9cm]{img/analogique.png}
\caption{Réponse impulsionelle en fonction de la fréquence}
\label{analogique}
\end{figure}

Un filtre \textit{réel} a un $|H(j\omega)|$ qui ressemblent plus à la figure \ref{analogique}. Le \textbf{gabarit} précise les \textit{tolérances} qu'on accepte quand on design un filtre.
\begin{enumerate}
\item \textcolor{red}{Fluctuations bande passante}: on détermine un $\epsilon$ qui définit la largeur de bande passante.
\item \textcolor{red}{Zone de transition}: correspond au $\Delta$ qui indique la largeur de cette zone.
\item \textcolor{red}{Fluctuations bande bloquante}: la largeur de cette bande est définit via $\delta$.
\end{enumerate}

\subsubsection{Les filtres passe-bas}
\begin{center}
\begin{tabular}{|m{3cm}|m{5cm}|m{7cm}|}
\hline
\cellcolor[gray]{0.8} Nom &\cellcolor[gray]{0.8} Réponse impulsionnelle & \cellcolor[gray]{0.8} Avantages \& Inconvénients\\
\hline
Butterworth & $|H(j\omega)|^2 = \frac{1}{1+(\omega/\omega_c)^{2n}}$ & On a un filtre avec une \textit{fluctuation de bande passante} minime. Une \textit{zone de transition} plus ou moins élevé et une \textit{bande bloquante} qui tend vers 0.\\
\hline
Tchebychev Direct & $|H(j\omega)|^2 = \frac{1}{1+(\gamma T_n^2 (\omega /\omega_c)}$ &$\gamma$ est le \textcolor{blue}{facteur d'ondulation} et $T_n^2$ est une fonction oscillante à amplitude \textit{constante}(\textit{polynôme de Tchebychev de première espèce d'ordre $n$}). On a un filtre avec une \textit{fluctuation de bande passante} oscillante mais borné. Une \textit{zone de transition} rapide et une \textit{bande bloquante} qui tend vers 0 et plus vite que \textcolor{blue}{Butterworth}.\\
\hline
Tchebychev Inverse & $|H(j\omega)|^2 = \frac{\sqrt{\gamma}T_n(\omega_c/\omega}{1+(\gamma T_n^2 (\omega /\omega_c)}$ &$\gamma$ est le \textcolor{blue}{facteur d'ondulation} et $T_n^2$ est une fonction oscillante à amplitude \textit{constante}(\textit{polynôme de Tchebychev de première espèce d'ordre $n$}). On a un filtre avec une \textit{fluctuation de bande passante} plate. Une \textit{zone de transition} rapide et une \textit{bande bloquante} oscille de manière bornée (l'inverse de \textcolor{blue}{Tchebychev}).\\
\hline
Elliptiques/Cauer & $|H(j\omega)|^2 = \frac{1}{1+ \gamma R_n^2(\omega / \omega_n \zeta)}$ &$\gamma$ est le \textcolor{blue}{facteur d'ondulation}, $\zeta$ est le \textcolor{blue}{facteur de sélectivité} et $R_n^2$ est une fonction \textcolor{blue}{rationnelle elliptique}. On a donc un filtre où en modifiant les paramètres $\gamma$ et $\zeta$, on peut obtenir un filtre spécifique.\\
\hline
\end{tabular}
\end{center}

\subsubsection{Les filtres passe-haut}
\begin{center}
\begin{tabular}{|m{3cm}|m{5cm}|m{7cm}|}
\hline
\cellcolor[gray]{0.8} Nom &\cellcolor[gray]{0.8} Réponse impulsionnelle & \cellcolor[gray]{0.8} Avantages \& Inconvénients\\
\hline
Butterworth & $|H(j\omega)|^2 = \frac{1}{1+(\omega/\omega_c)^{2n}}$ & Est très similaire à son homologue passe-bas. Donc on a des bandes très plate et une zone de transition assez rapide dépendant de son degré.\\
\hline

\end{tabular}
\end{center}

\subsubsection{Les filtres passe-bande}
Pour obtenir un filtre passe-bande, on combine 2 filtres passe-bas. On doit réaliser le changement de variable suivant:
\begin{align*}
j\omega \rightarrow \frac{\omega_0^2 - \omega^2}{Bj\omega}
\end{align*}
Où $B$ est la largeur du filtre souhaitée et $\omega_0$ est la fréquence à laquelle on va centrer nos deux bandes (une \textit{positive} et \textit{négative}).

\subsubsection{Les filtres à encoche / notch}
Ce sont des filtres qui ne font passer qu'une seule fréquence \textit{spécifique}.
\begin{align}
H(j\omega) = \frac{(j\omega + j\omega_0)(j\omega - j\omega_0)}{(j \omega + \omega_0(cos(\theta) + j\sin(\theta))(j \omega + \omega_0(cos(\theta) -j\sin(\theta))) }
\end{align}

\subsubsection{Les filtres passe-tout}
Il existe également une famille de filtre qui fait \textit{tout passer}. Son utilité ? \textcolor{blue}{déphaser} de $+/- \pi$ un signal ce qui peut être fort utile !
\begin{equation}
H(j\omega) = \frac{(j\omega - 2)}{(j\omega + 2)}
\end{equation}


\section{Caractérisation d'un filtre}
Pour se faire, on prend son équation de réponse impulsionnelle et on trace son diagramme de \textcolor{blue}{Bode}. (ne pas oublier que Bode c'est \textit{la norme} \textbf{et} \textit{l'argument})\\

Pour se faire, on utilise une échelle \textit{logarithmique}. On doit également \textcolor{blue}{factoriser} notre réponse impulsionnelle comme suit:
\begin{align}
|H(j\omega)| &= K \frac{|1 + \frac{j\omega}{\omega{z_1}}||\frac{j\omega}{\omega_{z_2}}|\bigr|(\frac{j\omega}{\omega_{z_3}})^2 + 2 \xi \frac{j\omega}{\omega_{z_3}}+1\bigr|}{|1 + \frac{j\omega}{\omega{p_1}}||\frac{j\omega}{\omega_{p_2}}|\bigr|(\frac{j\omega}{\omega_{p_3}})^2 + 2 \xi \frac{j\omega}{\omega_{p_3}}+1\bigr|}\\
20 &log(|H(j\omega)|)
\end{align}
Grâce aux propriétés des logarithmes, on peut séparer et additionner les multiplications et et divisions.\\
Ainsi, on trace morceau par morceau notre fonction et on va sommer ses différentes parties.
\begin{align*}
\textcolor{orange}{20 log|K|} + \textcolor{red}{20 log \bigr| \frac{j\omega}{\omega_{z_1}}\bigr|} + \textcolor{blue}{20 log(\bigr|1 + \frac{j\omega}{\omega_{z_2}}\bigr|)}
\end{align*}

\begin{figure}[H]
\centering
\includegraphics[width=4.5cm]{img/K.png}
\includegraphics[width=5cm]{img/jomega.png}
\includegraphics[width=5cm]{img/jomegacarre.png}
\end{figure}

et pour la version au \textit{carré} on a ceci. Il faut faire attention aux \textbf{asymptotes} qui peuvent se former !
\begin{align*}
\textcolor{green}{20log(\bigr|  \bigr(\frac{j\omega}{\omega_{z_3}}\bigr)^2 + 2 \xi \frac{j\omega}{\omega_{z_3}} +1 \bigr|)}
\end{align*}
\begin{figure}[H]
\centering
\includegraphics[width=6cm]{img/jomegamax.png}
\end{figure}
Après cela, on additionne/soustrait simplement nos courbes ensembles et on obtient notre \textit{première partie} du diagramme de \textcolor{blue}{Bode}.

\subsection{Argument}
Pour l'argument, le processus est le même si ce n'est qu'on ne met pas l'argument dans un logarithme mais est représenté sur une échelle logarithmique.\\
On trace nos différents arguments et on les somme.

\chapter{L'échantillonnage}
L'échantillonnage est quelque chose qui est partout autour de nous. C'est le fait de transformer un signal continu (\textit{son, vidéo, ...}) en un signal discret. (afin de le stocker sur un pc, ...) C'est donc un processus vitale à l'ère de l'informatique.
\section{Lien entre le continu et le discret}
Donc, pour sauvegarder un signal en discret, il faut \textit{ponctuellement} le sauvegarder.On appelle cela \textcolor{blue}{l'échantillonnage}

\subsection{L'échantillonnage}
\begin{wrapfigure}{r}{.5\textwidth}
\centering
\begin{align*}
\text{Pas d'échantillonnage: } &T_e\\
\text{Fréquence d'échnatillonnage: } & f_e = \frac{1}{T_e}\\
x[n] &= x(nT_e)\\
x_e(t)=x(t) \sum_{n \in \mathbb{Z}} \delta(t-nT_e) &= \sum_{n \in \mathbb{Z}} x(nT_e)\delta(t-nT_e)
\end{align*}
\end{wrapfigure}
On relève le signal discret toutes les $T_e$ secondes. Notre signal discret correspond à $x[n]$. Donc la transformation continu $\rightarrow$ est \textit{trivial}.\\
Une fois qu'on a notre signal \textit{discret}, on peu le retransformer en un signal discret ! En effet, on peut lier $x(t)$ à un peigne/train de Dirac.\\
On remarque que $x(nT_e) = x[n]$.

\subsubsection{Transformé de Fourier via continu}
Pour trouver la transformer de Fourier de notre $x_e(t)$ on réalise ceci:
\begin{align*}
x_e(t)&= x(t) \sum_{n \in \mathbb{Z}} \delta(t-nT_e)\\
X_e(j\omega) &= \frac{1}{2\pi} \Bigr[ X(j\omega) \ast \frac{2\pi}{T_e} \sum_{k \in \mathbb{Z}} \delta(\omega- \frac{2\pi}{T_e}k) \Big] \qquad  \frac{2\pi}{T_e} = \omega_e\\
&= \frac{1}{T_e} \Bigr[ X \ast \sum_{k \in \mathbb{Z}} \delta(-\omega_e) \Bigr](j\omega)\\
&= \frac{1}{T_e} \sum_{k \in \mathbb{Z}} X(j(\omega -k \omega_e))
\end{align*}
\newpage
\begin{wrapfigure}{r}{.5\textwidth}
\centering
\includegraphics[width=7cm]{img/rep.png}
\end{wrapfigure}
Donc notre $\omega_e$ représente la période. On observe également une différence entre $|X(j\omega)|$ et $|X_e(j\omega)|$. En effet, on peut remarquer une répétition tous les $\omega_e = 2 \pi f_e$. Cette contrainte va être très importante pour le taux d'échantillonnage et pour le repli spectral (\ref{spec})

\subsubsection{Transformé de Fourier via discret}
\begin{align*}
x_e(t)&= \sum_{n \in \mathbb{Z}} x[n]\delta(t-nT_e)\\
X_e(j\omega) &= \sum_{n \in \mathbb{Z}} x[n] e^{-j \omega n T_e}\\
&= X(e^{j\Omega})\vert_{\Omega = \omega T_e}
\end{align*}
On retrouve également ce phénomène de \textit{répétition} pouvant mener à un enchevêtrement.

\begin{figure}[H]
\centering
\includegraphics[width=10cm]{img/dua.png}
\caption{Dualité de Fourier}
\end{figure}

\section{Repli spectral} \label{spec}
Comme notre transformé de Fourier se répète tous les $\omega_e$, cela indique que le pas de temps \textit{d'échantillonnage} est primordial. Un pas trop \textit{petit} et les répétitions se \textcolor{red}{superposent}. On appelle cela le \textcolor{blue}{repli spectral}(\textit{aliasing}).\\
On peut penser aux roues d'un voiture, qui si le nombre de \textit{FPS} n'est pas assez élevé pour le nombre de rotation par seconde, nous donnera l'impression qu'elle tourne à l'envers.

\subsection{Bien échantillonner}
Avec l'idée de la \textit{voiture}, on peut déjà poser le fait qu'un bon échantillonnage est $2T_e \leq T$. Ainsi on verra la rotation.

\subsubsection{Théorème de Shannon} \label{Sha}
Une fonction (à bande limitée) qui ne contient pas de fréquences supérieures à $f_{max}$ est complètement déterminée par ses échantillons pour autant que:
\begin{align*}
f_e \geq 2 f_{max} \longleftrightarrow \omega_e \geq 2 \omega_{max}
\end{align*}	

\subsection{Reconstruire un signal}
Si le théorème de Shanon (\ref{Sha}) est bien respecté, on peut donc reconstituer un signal en \textit{continu} en filtrant son \textit{spectre discret}.

\begin{align*}
X(j\omega) = X_e(j\omega) T_e \sqcap \Bigr( \frac{\omega}{\omega_c} \Bigr) \qquad (\omega_max \leq \omega_c \leq \omega_e - \omega_max)
\end{align*}

Il existe également une formule s'intitulant la \textcolor{blue}{formule de Shanon}:

\begin{align*}
\sum_{n \in \mathbb{Z}} x[n] sinc\Bigr( \frac{t-nT_e}{T_e} \Bigr)
\end{align*}
L'utilité du sinus \textcolor{blue}{cardinal} est qu'il interpole les points $(t, x(nT_e))$. Il existe d'autres fonctions d'interpolations qui donnent des résultats différents.
\begin{figure}[H]
\centering
\includegraphics[width=10cm]{img/int.png}
\caption{Différentes interpolations}
\end{figure}

\subsection{Généralisation de Shanon}
Donc de manière plus général, on peut reconstruire tous signaux tant qu'il n'y a pas de recouvrement spectral. Par exemple des signaux plus complexes possédant \textbf{plusieurs} fréquences et qui sont de formes variées. On doit simplement respecter la formule de Shanon et bien lier la fréquence positive et sa fréquence négative. (\textit{dualité fréquentielle})

\section{Sur- et Sous-échantillonnage}


\part{Systèmes}

\chapter{Système LIT}
Un système est une entité qui prend \textit{un ou plusieurs signaux} en entrée et produit \textit{de nouveaux signaux} en sortie.\\
Exemple: $H\{x[n]\} = x[n] + x[n-1]$. Un système est par exemple: \textit{une radio, une caméra, une voiture, ...}

\section{LIT}
Un système \textit{Linéaire et Indépendant du Temps} ou \textbf{LIT} est, comme son nom l'indique, linéaire donc:
\begin{equation}
\mathcal{H}\{a_1x_1 + ... + a_Nx_N\} = a_1\mathcal{H}\{x_1\} + ... + a_N\mathcal{H}\{x_N\} 
\end{equation}
Et un système est \textit{invariant temporelle} donc:
\begin{equation}
\begin{cases}
\mathcal{H} \text{ est invariant si } \forall t_0 \in \mathbb{N}\\
\mathcal{H}\{x\}[n] = y[n] \Rightarrow \mathcal{H}\{x[n-n_0]\} = y[n-n_0]
\end{cases}
\end{equation}
On remarque qu'on peut ré-écrire tous signaux via une somme d'impulsions. De plus, si $\mathcal{H}$ est linéaire alors:
\begin{equation}
\mathcal{H}\{x\} = \mathcal{H}\biggl\{\sum_{k= -\infty}^{\infty} x[k]\delta[n-k] \biggl\} = \sum_{k=-\infty}^{\infty} x[k]\mathcal{H}\{\delta[n-k]\}
\end{equation}
Si $\mathcal{H}$ est \textit{invariant} au temps et qu'on pose $h := \mathcal{H}\{\delta\}$
\begin{align}
\mathcal{H}\{\delta[n-k]\} = \mathcal{H}\{\delta\}[n-k] &= h[n-k]\\
\mathcal{H}\{x\} = \sum_{k=-\infty}^{\infty} x[k]h[n-k] &=: x \ast h
\end{align}
A noter que "$\ast$" fais référence à la convolution, sujet abordé à la section \ref{convo}.\\

Il est important de noter que toutes ces propriétés et caractéristiques des systèmes \textit{LIT} en \textbf{temps discret} sont également valables et ont un équivalent en \textbf{temps continu}.



\section{Opération sur les signaux}
Voici un tableau résumant les différentes opérations possibles sur les signaux:
\begin{center}
\begin{tabular}{|c|c|c|}
	\hline
	 & Temps discret & Temps continu\\
	\hline
	Combinaison linéaire & $\alpha_1 x_1[n] + \alpha_2 x_2[n]$ & $\alpha_1 x_1(t) + \alpha_2 x_2(t)$\\
	\hline
	Multiplication & $x_1[n]x_2[n]$ & $x_1(t)x_2(t)$\\
	\hline
	Différentiation &  & $d^nx(t)/dt^n$\\
	\hline
	Intégration &  & $\int_{-\infty}^t x(\tau)d\tau$\\
	\hline
	\textbf{Convolution} & $x_1[n] \ast x_2[n]$ & $x_1(t) \ast x_2(t)$\\
	\hline
	Dilatation & $x[n/a]$ (arrondi $n/a$) & $x(t/a) a>0$\\
	\hline
	Translation & $x[n-n_0], n_0 \in \mathbb{Z}$ & $x(t-t_0), t_0 \in \mathbb{R}$\\
	\hline
	Renversement & $x[-n]$ & $x(-t)$\\
	\hline
\end{tabular}
\end{center}
Une chose importante à voir dans ces formules est que $x$ est un \textit{signal}, $x[k]$ la \textit{valeur} de ce signal en k et on a $\mathcal{H}$ qui est un \textit{système} prenant et donnant des signaux.

\section{Convolution (temps discret)} \label{convo}
La convolution est un nouvel opérateur qui nous sera très utile. Son signe est "$\ast$" et la formule qui définit cette opération est:
\begin{equation} \label{eqn:convolution}
f[n] \ast g[n] := \sum_{k=-\infty}^{\infty} f[k]g[n-k] 
\end{equation}
La méthode pour trouver le résultat d'une convolution de manière \textbf{graphique} est:
\begin{enumerate}
\item Il faut "\textit{renverser}" une des fonctions. C'est-à-dire faire $f[n] \rightarrow f[-n]$.
\item On décale une des fonctions le plus à droite. (on prend le $k_0$ où après, tous les résultats de $f[k]g[n-k]$ valent $0$) 
\item Puis multiplier chaque point entre eux et les sommer.
\item On met le résultat sur un \textit{graphe} au point $k$.
\item On décale notre fonction d'un point vers la gauche et on répète le processus.
\end{enumerate}
Pour trouver de manière \textbf{calculatoire}, on applique simplement la formule \ref{eqn:convolution}.

\subsection{Propriétés}
\begin{center}
\begin{tabular}{c|c}
	Commutativité & $(f\ast g)[n] = (g \ast f)[n]$ \\
	\hline
	Associativité & $(f \ast (g \ast h))[n] = ((f \ast g) \ast h)[n]$ \\
	\hline
	Distributivité & $(f \ast (g + h))[n] = (f \ast g + f \ast h)[n]$ \\
\end{tabular}
\end{center}
On également ces propriétés:
\begin{center}
\begin{tabular}{c|c}
	Élément neutre & $f[n] \ast \delta[n] = f[n]$ \\
	\hline
	Décalage & $f[n] \ast \delta[n-n_0] = f[n-n_0], n_0 \in \mathbb{Z}$ \\
\end{tabular}
\end{center}

\section{Convolution (temps continu)}
Si on a \textit{2 signaux} $f(t)$ et $g(t)$ leur convolution est donnée par (ressemble très fort à \ref{convo}):
\begin{equation}
f(t) \ast g(t) = \int_{-\infty}^{\infty}f(\tau)g(t-\tau)d\tau
\end{equation}
Pour calculer de manière \textbf{graphique} il faut suivre ces étapes:
\begin{enumerate}
\item Il faut "\textit{renverser}" une des fonctions. C'est-à-dire faire $f(t) \rightarrow f(-t)$.
\item On décale une des fonctions le plus à droite. (on prend le $\tau_0$ où après, tous les résultats de $f(t)g(t-\tau_0)$ valent $0$) 
\item Puis multiplier chaque point entre eux et on les intègre. (on prend la surface sous la courbe).
\item On met le résultat sur un \textit{graphe} au pont $\tau$.
\item On décale notre fonction d'une distance (pas trop loin mais pas trop proche pour ainsi avoir une nuée de points) vers la gauche et on répète le processus.
\end{enumerate}
En somme, nous avons une méthode très proche du temps discret si ce n'est l'utilisation d'intégrale allant de $-\infty$ à $\infty$.

\subsection{Propriétés}
On retrouve les mêmes propriétés que en temps discret.
\begin{center}
\begin{tabular}{c|c}
	Commutativité & $(f\ast g)(t) = (g \ast f)(t)$ \\
	\hline
	Associativité & $(f \ast (g \ast h))(t) = ((f \ast g) \ast h)(t)$ \\
	\hline
	Distributivité & $(f \ast (g + h))(t) = (f \ast g + f \ast h)(t)$ \\
\end{tabular}
\end{center}
On également ces propriétés:
\begin{center}
\begin{tabular}{c|c}
	Élément neutre & $f(t) \ast \delta(t) = f(t)$ \\
	\hline
	Décalage & $f(t) \ast \delta(t-t_0) = f(t-t_0), n_0 \in \mathbb{R}$ \\
\end{tabular}
\end{center}

\section{Réponse impulsionnelle}
La réponse impulsionnelle d'un système \textit{LIT} $\mathcal{H}$ est la réaction du système à une \textit{impulsion d'entrée} ($\delta[0]$) on le note $h$.\\

Pour \textbf{tout} système \textit{LIT}, le signal de sortie est le résultat de la convolution entre \textit{le signal d'entrée} et \textit{la réponse impulsionnelle}.
\begin{align}
y[n] &= x[n] \ast h[n]\\
y(t) &= y(t) \ast h(t)
\end{align}

\section{Type de système}
\begin{center}
\begin{tabular}{|c|c|}
	\hline
	Système sans mémoire & Si la sortie du système \textit{à un temps donné}\\
	&  ne dépend que de l'entrée à \textbf{cet instant}.\\
	\hline
	Système causal & Le système \textbf{ne dépend pas} de ce qui se passe dans le \textit{futur}\\
	\hline
	Système stable ou (\textit{BIBO}) & Entrée \textit{bornée} donne une sortie \textit{bornée}\\
	\hline
	Système inversible & On sait \textit{retrouver} l'entrée en ayant la \textit{sortie}.\\
	\hline

\end{tabular}
\end{center}

\section{Modélisation et représentation des systèmes}
Comme vu précédemment, la réponse impulsionnelle est le résultat du système étant perturbé par une impulsion. 
\subsection{Inconvénient}
\begin{enumerate}
\item Fonction de taille \textit{infinie} et représentation donc \textit{peu simple}.
\item La modélisation d'un système ne mène généralement pas à une réponse impulsionnelle.
\item On doit connaitre l'entrée depuis $-\infty$.
\end{enumerate}


\subsection{Représentation}
\begin{wrapfigure}{r}{.3\textwidth}
\includegraphics[width=5cm]{img/RLC.png}
\caption{Circuit RLC}
\label{fig:RLC}
\end{wrapfigure}
Il existe 3 grandes façons de représenter un système. Tout d'abord la méthode \textit{équation différentielle d'entrée-sortie}. Pour la suite des exemples, j'utiliserai un circuit \textit{RLC} comme montré ci-contre.\\

\textit{équation différentielle d'entrée-sortie} est une somme des dérivées comme montré dans l'équation \ref{eqn:es}. C'est plutôt facile de trouver les équations mais on fait face à un problème, l'opération \textit{dérivée} n'existe pas dans le monde réelle, il faut un opérateur intégrateur.\\

Ensuite, nous avons la \textit{représentation d'état} qui utilise des matrices pour former les équations différentielles comme nous voyons à l'équation \ref{eqn:mat}\\

La dernière représentation type est le \textit{schéma bloc} qui est visuel et qui utilise lui des blocs intégrateurs au lieu de dérivé comme montré à la figure \ref{fig:bloc}.

\begin{equation} \label{eqn:es}
\ddot{y} + \frac{1}{CR}\dot{y} + \frac{1}{LC}y = \frac{1}{CR}\dot{u}
\end{equation}

\begin{equation} \label{eqn:mat}
\frac{d}{dt}\begin{pmatrix}
V_0\\
I_L
\end{pmatrix} = \begin{pmatrix}
-1/RC & -1/C\\
1/L & 0
\end{pmatrix} \begin{pmatrix}
V_0\\
I_L
\end{pmatrix} + \begin{pmatrix}
1/RC\\
0
\end{pmatrix} u
\end{equation}

\begin{figure}[H] 
\centering
\includegraphics[width=7cm]{img/bloc.png}
\caption{Repésentation \textit{bloc} du système à la figure \ref{fig:RLC}}
\label{fig:bloc}
\end{figure}

\subsection{Équation différentielle entrée-sortie}
La forme générale de ces équations est de ce type:
\begin{equation}\label{eqn:difes}
\sum_{k = 0}^{N} a_k (\frac{d^k}{dt^k}) y(t) = \sum_{k = 0}^M b_k (\frac{d^k}{dt^k}) u(t)
\end{equation}
Quelque chose à bien remarquer est que ces équations ne modélise \textit{qu'une partie} d'un système \textit{LIT}. Par exemple, on ne peut pas représenter un \textit{délai} ce qui également rare dans la réalité.\\

Une chose à remarquer est que l'opérateur \textit{dérivé} peut se "\textit{démultiplier}" et possède une \textit{associativité} et \textit{commutativité}. Ainsi, on peut avoir une représentation dite "\textit{polynomiale}" comme ci-dessus qui est une autre écriture de l'équation \ref{eqn:difes}.
\begin{equation}\label{eqn:poly}
p(\frac{d}{dt})y(t) = q (\frac{d}{dt})u(t)
\end{equation}
Puis après, pour résoudre ce genre d'équation, on utilise des méthodes classiques vu au cours d'Analyse donc, solution homogène et particulière ...

\subsubsection{Réponse libre et forcée}
Une réponse \textit{libre} est la solution de l'équation homogène, donc quand $u(t) = 0$ à l'équation \ref{eqn:poly} et on garde les \textbf{mêmes conditions initiales}. En somme c'est la représentation de l'impact des \textit{CI}.\\

Une réponse \textit{forcée} est l'équation \ref{eqn:poly} mais avec les \textit{conditions initiales} nulles. Donc on s'intéresse à l'impact de l'entrée sans les conditions initiales. La somme de la réponse \textit{libre} et \textit{forcée} nous donne la réponse générale.

\subsubsection{Stabilité}
On peut avoir une intuition sur la stabilité de notre système en posant $y_H(t)$ qui équivaut à:
\begin{equation}
y_H(t) = \sum_i \alpha_ie^{r_it} \rightarrow \quad r_i \text{ correspond aux racines de p(z) de \ref{eqn:poly}}
\end{equation}
Si la partie réelle de $r_i < 0 \forall i$ alors on a une exponentielle décroissante donc \textbf{stable}. On appelle ce genre de système \textit{BIBO} stable ou \textit{Bounded Input Bounded Output}.\\

En revanche, si $r_i > 0 \exists i$ donc on a au moins une exponentielle croissante créant une \textit{instabilité}.\\

Si $r_i = 0 \exists i$ on dit qu'on a une \textit{stabilité marginale} ou \textit{instabilité}. Cela dépendera de la \textbf{multiplicité} et de \textbf{$te^{r_it}$}.

\subsubsection{Linéarité de l'entrée}
Avec cette représentation polynomiale, on peut facilement voir qu'on a une linéarité de l'entrée nous permettant de simplifier différent calcul. %rajouter la formule etc

\subsubsection{Avantages et Inconvénients}
2 représentations qui sont \ref{eqn:difes} et \ref{eqn:poly}. Les avantages:
\begin{itemize}
\item Représentation compacte.
\item Conditions initiales claires.
\item Facile de la transformer dans d'autres représentations.
\end{itemize}
Les désavantages:
\begin{itemize}
\item On peut perdre la représentation physique.
\end{itemize}

\subsection{Schéma Bloc}
Comme son nom l'indique, le schéma bloc utilise des "\textit{blocs}" pour représenter notre système. Ci-dessus on peut voir les composants de base composant ce type de schéma
\begin{figure}[H]
\centering
\includegraphics[width=5cm]{img/blocExemple.png}
\caption{Liste reprenant les blocs de base}
\end{figure}
Dans le cadre des systèmes \textit{LIT}, on se restreint souvent à l'addition, multiplication et intégration. On ne réalise que des opérations \textit{linéaires}.

\subsubsection{Avantages et Inconvénients}
Les avantages:
\begin{itemize}
	\item Représentation très intuitive.
	\item Proche de l'implémentation réelle du système.
	\item On peut plus facilement réfléchir sur le \textit{design} de notre système.
	\item Très modulaire.
\end{itemize}
Les désavantages:
\begin{itemize}
	\item Lien moins clair avec la solution.
\end{itemize}
De plus, on peut voir comme un \textit{avantage} ou \textit{inconvénient} le fait de pouvoir voir l'évolution des signaux entre chaque bloc plutôt qu'une sorte de boite noire "\textit{entrée-sortie}". De plus, on peut réaliser de bien des manières des circuits.

\subsection{Représentation d'état}
Dans ce type de représenation, on a \textcolor{red}{état $x$} qui est un \textit{vecteur} comportant toutes les infos internes de notre système. On \textcolor{red}{entrée $u$} qui est un \textit{signal} extérieur affectant le système. Finalement on a une \textcolor{red}{sortie $y$} qui est un signal qu'on peut accéder depuis l'extérieur.\\
\begin{equation}
\begin{cases}
\text{évolution: }\frac{d}{dt}x(t) =  Ax(t) + Bu(t)\\
\text{sortie: } C x(t) + D u(t)
\end{cases}
\end{equation}

\subsubsection{Solution}
La solution \textit{homogène} de $e^{\lambda_it}$ avec $\lambda_i$ étant les valeurs propres de l'équation. Si la partie réelle des racines est négative pour tout $\lambda_i$.

\subsubsection{État non-unique}
Avec cette représentation, on peut facilement modifier les vecteurs et signaux pour rendre les équations plus simples. Cela n'a aucun impacte et rend les équations plus logiques pour un certain sens "\textit{réel}".
\begin{equation} \label{eqn:nonunique}
\begin{cases}
z = Tx \Rightarrow \frac{d}{dt}z = T\frac{d}{dt}x = TAT^{-1}z(t) + TBu(t)\\
y = Cx(t) + Bu(t) = CT^{-1}z(t) + Du(t) 
\end{cases}
\end{equation}

Si la matrice $A$ est diagonalisable, on peut réaliser un \textbf{découplage} et obtenir ainsi un mode dit \textit{découplé}. On peut également faire des \textit{blocs de Jordan} pour diagonaliser le tout:
\begin{equation}
\frac{d}{dt} z_i = \lambda_i z_i + \tilde{B}_i u
\end{equation}

\subsection{Passage de représentation}
% à faire et ajouter les graphes
\subsection{Temps discret}
Pour passer du temps continu au temps discret, il faut transformer $\frac{d}{dt}$ en l'opérateur de \textit{décalage} $D$:
\begin{equation}
Dx[n] =  x[n-1]
\end{equation}
Ce qui nous permet d'établir l'équation de \textit{différence}
\begin{equation}
\begin{cases}
\sum_{k=0}^N a_k y[n-k] = \sum_{k=0}^M b_ku[n-k]\\
p(D)y = q(D)u
\end{cases}
\end{equation}
Ce qui nous donne pour solution homogène:
\begin{equation}
y_H [N] = \sum_i c_i r_i^n
\end{equation}
On a une décroissance donc \textit{stabilité} si $|r_i| < 1$ et une croissance si $|r_i| > 1$\\
De plus, on remplace le bloc \textit{intégrateur} du temps discret en bloc $D^{-1}$. La ré-écriture de l'équation \ref{eqn:nonunique} en temps discret:
\begin{equation}
\begin{cases}
x[n+1] = Ax[n] + Bu[n]\\
y[n] = Cx[n] + Du[n]
\end{cases}
\end{equation}
On approxime un système en temps \textit{continu} en temps \textit{discret} ssi:
\begin{equation}
A_d \approx A_c \Delta t \text{ si } \Delta t \text{ petit}
\end{equation}

\subsection{Résumé}
\begin{enumerate}
\item Réponse impulsionnelle, universelle mais peu maniable $\Rightarrow$ On voit que l'entrée et sortie et \textbf{Pas de CI}
\item Équation différentielle entrée-sortie $\Rightarrow$ On voit que l'entrée et sortie et on a des \textbf{CI}
\item Représentation d'état (matrice) $\Rightarrow$ On voit l'intérieur et on a des \textbf{CI}
\item Schéma Bloc (très concret) $\Rightarrow$ On voit l'intérieur et on a des \textbf{CI}
\end{enumerate}

\subsection{Existence des systèmes LIT}
Une forme usuelle des systèmes LIT dans la vraie vie est de type $\dot{x}(t) = f(x(t), u(t), t)$.\\
\textbf{Invariance temporelle}: tout système fait face à l'usure mais on estime que sur la période d'observation, l'usure est minime et peut être ignorée.\\
\textbf{Linéarité}: aucun système n'est pas linéaire. Cela peut être dû à des \textit{imperfections} ou si on augmente \textit{énormément} l'entrée ce qui change le comportement du système.\\



\end{document}
